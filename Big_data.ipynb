{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f66a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015ccb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  #최대=1, 최소=0 되도록 스케일링\n",
    "\n",
    "covtype = fetch_covtype(shuffle=True, random_state=0)\n",
    "X_covtype = covtype.data\n",
    "y_covtype = covtype.target - 1\n",
    "classes = np.unique(y_covtype)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_covtype, y_covtype)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def read_Xy(start, end):\n",
    "    # 실무에서는 파일이나 데이터베이스에서 읽어온다.\n",
    "    idx = list(range(start, min(len(y_train) - 1, end)))\n",
    "    X = X_train[idx, :]\n",
    "    y = y_train[idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fe0b2",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7508971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train acc=0.708 test acc=0.707\n",
      "epoch=1 train acc=0.712 test acc=0.712\n",
      "epoch=2 train acc=0.713 test acc=0.713\n",
      "epoch=3 train acc=0.714 test acc=0.713\n",
      "epoch=4 train acc=0.713 test acc=0.712\n",
      "epoch=5 train acc=0.713 test acc=0.712\n",
      "epoch=6 train acc=0.713 test acc=0.711\n",
      "epoch=7 train acc=0.712 test acc=0.711\n",
      "epoch=8 train acc=0.712 test acc=0.711\n",
      "epoch=9 train acc=0.712 test acc=0.711\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SGDClassifier(random_state=0)\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    for n in range(n_split):\n",
    "        X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "        model.partial_fit(X, y, classes=classes)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train acc={:5.3f} test acc={:5.3f}\".format(epoch, accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b206d79",
   "metadata": {},
   "source": [
    "## 나이브베이즈 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb45c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=0 train accuracy=0.634 test accuracy=0.631\n",
      "n=1 train accuracy=0.634 test accuracy=0.631\n",
      "n=2 train accuracy=0.633 test accuracy=0.630\n",
      "n=3 train accuracy=0.633 test accuracy=0.629\n",
      "n=4 train accuracy=0.633 test accuracy=0.630\n",
      "n=5 train accuracy=0.633 test accuracy=0.630\n",
      "n=6 train accuracy=0.632 test accuracy=0.629\n",
      "n=7 train accuracy=0.634 test accuracy=0.631\n",
      "n=8 train accuracy=0.632 test accuracy=0.629\n",
      "n=9 train accuracy=0.632 test accuracy=0.629\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = BernoulliNB(alpha=0.1)\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test)) \n",
    "    print(\"n={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e91ffa",
   "metadata": {},
   "source": [
    "## 그레디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8767e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2205\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score -1.006656\n",
      "[LightGBM] [Info] Start training from score -0.716293\n",
      "[LightGBM] [Info] Start training from score -2.807880\n",
      "[LightGBM] [Info] Start training from score -5.369033\n",
      "[LightGBM] [Info] Start training from score -4.104378\n",
      "[LightGBM] [Info] Start training from score -3.529187\n",
      "[LightGBM] [Info] Start training from score -3.347257\n",
      "n=0 train accuracy=0.788 test accuracy=0.785\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2204\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "n=1 train accuracy=0.810 test accuracy=0.805\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2206\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "n=2 train accuracy=0.804 test accuracy=0.799\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2205\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "n=3 train accuracy=0.802 test accuracy=0.795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2200\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=4 train accuracy=0.796 test accuracy=0.789\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2199\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=5 train accuracy=0.796 test accuracy=0.788\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2204\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=6 train accuracy=0.796 test accuracy=0.787\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 49\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=7 train accuracy=0.793 test accuracy=0.784\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2204\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=8 train accuracy=0.790 test accuracy=0.782\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2204\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=9 train accuracy=0.780 test accuracy=0.773\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from lightgbm import train, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    \"num_class\": len(classes),\n",
    "    'learning_rate': 0.2,\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree = 10\n",
    "model = None\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model = train(params, init_model=model, train_set=Dataset(X, y),\n",
    "                  keep_training_booster=False, num_boost_round=num_tree)\n",
    "    accuracy_train = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
    "    accuracy_test = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1)) \n",
    "    print(\"n={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007344b2",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train accuracy=0.867 test accuracy=0.853\n",
      "epoch=1 train accuracy=0.891 test accuracy=0.872\n",
      "epoch=2 train accuracy=0.899 test accuracy=0.879\n",
      "epoch=3 train accuracy=0.903 test accuracy=0.883\n",
      "epoch=4 train accuracy=0.905 test accuracy=0.885\n",
      "epoch=5 train accuracy=0.906 test accuracy=0.886\n",
      "epoch=6 train accuracy=0.907 test accuracy=0.887\n",
      "epoch=7 train accuracy=0.907 test accuracy=0.887\n",
      "epoch=8 train accuracy=0.908 test accuracy=0.888\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree_ini = 10\n",
    "num_tree_step = 10\n",
    "model = RandomForestClassifier(n_estimators=num_tree_ini, warm_start=True)\n",
    "for n in range(n_split):\n",
    "    X, y = read_Xy(n * n_X, (n + 1) * n_X)\n",
    "    model.fit(X, y)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train accuracy={:5.3f} test accuracy={:5.3f}\".format(n, accuracy_train, accuracy_test))\n",
    "    \n",
    "    model.n_estimators += num_tree_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ca23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0444f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fad446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bb7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55a9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
